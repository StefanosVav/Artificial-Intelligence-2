{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8iGZzlgWOlqs"
      },
      "source": [
        "# Developing a sentiment classifier by fine-tuning the pretrained BERT-base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_2EZpFUHTAA",
        "outputId": "3b08867f-1b8d-4014-e3b4-0c8daf83232b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1LDU9zyyU9p"
      },
      "source": [
        "Specify gpu as torch device if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ADe3u8jYMnA",
        "outputId": "a0f82078-9a29-4afa-dcbd-02800daf14ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('Using GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('Using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Tjtno3KdEr"
      },
      "source": [
        "# Store reviews.csv file to pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT1mzbUgOUX1",
        "outputId": "b184420e-7217-4304-b9f7-b71e4250513f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    url  rating  \\\n",
            "0      http://www.imdb.com/title/tt0120623/usercomments    10.0   \n",
            "1      http://www.imdb.com/title/tt0043117/usercomments     9.0   \n",
            "2      http://www.imdb.com/title/tt0043117/usercomments    10.0   \n",
            "3      http://www.imdb.com/title/tt0835204/usercomments     4.0   \n",
            "4      http://www.imdb.com/title/tt0499603/usercomments    10.0   \n",
            "...                                                 ...     ...   \n",
            "45003  http://www.imdb.com/title/tt0449000/usercomments     1.0   \n",
            "45004  http://www.imdb.com/title/tt0109382/usercomments     1.0   \n",
            "45005  http://www.imdb.com/title/tt0375560/usercomments     1.0   \n",
            "45006  http://www.imdb.com/title/tt0165107/usercomments     1.0   \n",
            "45007  http://www.imdb.com/title/tt0041513/usercomments    10.0   \n",
            "\n",
            "                                                  review  \n",
            "0      I thought this was a quiet good movie. It was ...  \n",
            "1      Wagon Master is a very unique film amongst Joh...  \n",
            "2      This film has to be as near to perfect a film ...  \n",
            "3      I gave this 4 stars because it has a lot of in...  \n",
            "4      This movie is really genuine and random. It's ...  \n",
            "...                                                  ...  \n",
            "45003  I don't even know where to begin...<br /><br /...  \n",
            "45004  One of the worst movies I saw in the 90s. I'd ...  \n",
            "45005  Baldwin has really stooped low to make such mo...  \n",
            "45006  If you liked watching Mel Gibson in Million Do...  \n",
            "45007  This is easily the best cinematic version of W...  \n",
            "\n",
            "[45008 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/gdrive/MyDrive/Artificial_Intelligence2/imdb-reviews.csv\", sep='\\t')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTOkJ4OKl8E"
      },
      "source": [
        "# Preprocess/cleanse data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgxnU-4mP7cM",
        "outputId": "0a356510-5717-43c8-b216-f41d7b364acb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-4-a8338bc6aa4e>:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['review'] = df['review'].str.replace('@[A-Za-z0-9_]+','')                                              # Remove mentions\n",
            "<ipython-input-4-a8338bc6aa4e>:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['review'] = df['review'].str.replace('[^\\w\\s]','')                                                     # Remove punctuation\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  thought quiet good movie fun watch liked best ...\n",
            "1        9.0  wagon master unique film amongst john fords wo...\n",
            "2       10.0  film near perfect film john ford made film mag...\n",
            "3        4.0  gave 4 stars lot interesting themes many alrea...\n",
            "4       10.0  movie really genuine random really hard find m...\n",
            "...      ...                                                ...\n",
            "45003    1.0  dont even know begin worth typing review quote...\n",
            "45004    1.0  one worst movies saw 90s id often use benchmar...\n",
            "45005    1.0  baldwin really stooped low make movies script ...\n",
            "45006    1.0  liked watching mel gibson million dollar hotel...\n",
            "45007   10.0  easily best cinematic version william faulkner...\n",
            "\n",
            "[45008 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def preprocess(df):\n",
        "  df = df.drop('url', axis = 1)\n",
        "  df['review'] = df['review'].str.lower()\n",
        "  df['review'] = df['review'].str.replace(\"<br />\", \" \")\n",
        "  df['review'] = df['review'].str.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)    # Remove urls\n",
        "  df['review'] = df['review'].str.replace('@[A-Za-z0-9_]+','')                                              # Remove mentions\n",
        "  df = df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))                  # Remove emojis\n",
        "  df['review'] = df['review'].str.replace('[^\\w\\s]','')                                                     # Remove punctuation\n",
        "  df['review'] = df['review'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))   # Remove stop words\n",
        "  return df\n",
        "\n",
        "df1 = preprocess(df)\n",
        "print(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvgeoGG7P_5S",
        "outputId": "768f5ca6-5abf-4886-e6bc-ba054f9f46e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "movie    79033\n",
            "film     69432\n",
            "one      46646\n",
            "like     35623\n",
            "good     26234\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "most_used_words = pd.Series(' '.join(df1['review']).split()).value_counts()[:5]\n",
        "print(most_used_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMzrhmvJK5LG"
      },
      "source": [
        "# Hold the reviews in dataframe X and the sentiment value (0 or 1) in dataframe Y\n",
        "\n",
        "Also replacing the review rating with sentiment 1 for positive, 0 for negative (>=7 to 1, <=4 to 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH0Ozo7jQG7P",
        "outputId": "0b0c3d2c-48cb-4688-9b95-ccf81180c3d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        thought quiet good movie fun watch liked best ...\n",
            "1        wagon master unique film amongst john fords wo...\n",
            "2        film near perfect film john ford made film mag...\n",
            "3        gave 4 stars lot interesting themes many alrea...\n",
            "4        movie really genuine random really hard find m...\n",
            "                               ...                        \n",
            "45003    dont even know begin worth typing review quote...\n",
            "45004    one worst movies saw 90s id often use benchmar...\n",
            "45005    baldwin really stooped low make movies script ...\n",
            "45006    liked watching mel gibson million dollar hotel...\n",
            "45007    easily best cinematic version william faulkner...\n",
            "Name: review, Length: 45008, dtype: object\n",
            "       rating\n",
            "0           1\n",
            "1           1\n",
            "2           1\n",
            "3           0\n",
            "4           1\n",
            "...       ...\n",
            "45003       0\n",
            "45004       0\n",
            "45005       0\n",
            "45006       0\n",
            "45007       1\n",
            "\n",
            "[45008 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "X = df1.drop('rating', axis=1)\n",
        "X = X.squeeze()\n",
        "Y = df1[['rating']]                 # only keep the rating\n",
        "Y = Y.replace(['7.0', '8.0', '9.0', '10.0'], 1)\n",
        "Y = Y.replace(['0.0', '1.0', '2.0', '3.0', '4.0'], 0)\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GkFPo_XOUsX",
        "outputId": "a4f08607-2f4a-4924-b347-a915b5d065a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4PGK9S8J5wR"
      },
      "source": [
        "# Import and load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zIJR1zNZOzga"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq9IoXrkJRvC"
      },
      "source": [
        "# Tokenize all reviews and map the tokens to their word IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E9mwclgXHEh",
        "outputId": "541cc0fa-6e80-4291-fa10-d9ac465e6ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review after preprocessing: wagon master unique film amongst john fords work mainly one based story written john ford story elaborated frank nugent directors son patrick ford turned screenplay directors personal opinion regarding wagon master film john ford called one came closest wanted achieve say say little ford confessed lindsay anderson favourite nonetheless darling clementine wagon master ingredients one might expect find john fords film wonderful cast delivering best thou featuring major stars except fordian actors ben johnson peculiar small characters provide obligatory comic relief wagon master quite horn blowing sister ledyard jane darwell shot inspired gigs last least legendary monument valley john fords fifth passage stagecoach darling clementine fort apache wore yellow ribbon film starts two friends cowboys travis blue ben johnson sandy owens harry carey jr hired wagon masters guides caravan mormon settlers headed silver valley place thats like promised land way joined peculiar dr locksley hall alan mowbray two beautiful women supposedly wife daughter call actors headed direction simply recently driven nearest town place go nothing particularly unpleasant happens till bump cleggs dangerous family gang consisting father three sons run marshal town recently committed murder bank robbery overall wagon master less one precious pearl necklace john fords wonderful westerns must see 910\n",
            "Mapped token IDs: tensor([  101,  9540,  3040,  4310,  2143,  5921,  2198,  4811,  2015,  2147,\n",
            "         3701,  2028,  2241,  2466,  2517,  2198,  4811,  2466, 25187,  3581,\n",
            "        16371, 11461,  5501,  2365,  4754,  4811,  2357,  9000,  5501,  3167,\n",
            "         5448,  4953,  9540,  3040,  2143,  2198,  4811,  2170,  2028,  2234,\n",
            "         7541,  2359,  6162,  2360,  2360,  2210,  4811, 14312, 12110,  5143,\n",
            "         8837,  9690,  9548, 12223,  3170,  9540,  3040, 12760,  2028,  2453,\n",
            "         5987,  2424,  2198,  4811,  2015,  2143,  6919,  3459, 12771,  2190,\n",
            "        15223,  3794,  2350,  3340,  3272,  4811,  2937,  5889,  3841,  3779,\n",
            "        14099,  2235,  3494,  3073, 26471,  5021,  4335,  9540,  3040,  3243,\n",
            "         7109, 11221,  2905,  2419, 14132,  4869, 18243,  4381,  2915,  4427,\n",
            "        20929,  2197,  2560,  8987,  6104,  3028,  2198,  4811,  2015,  3587,\n",
            "         6019, 26025,  9548, 12223,  3170,  3481, 15895,  5078,  3756, 10557,\n",
            "         2143,  4627,  2048,  2814, 11666, 10001,  2630,   102])\n",
            "Sentiment (Pos = 1, Neg = 0): tensor([1])\n"
          ]
        }
      ],
      "source": [
        "X_mapped = []\n",
        "att_masks = []\n",
        "\n",
        "for review in X:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                      review,\n",
        "                      add_special_tokens = True,          # Add [CLS] and [SEP] tokens\n",
        "                      padding = 'max_length',             # Pad and truncate all reviewss\n",
        "                      truncation = True,\n",
        "                      max_length = 128,                   # to max_length (Tried 64 and 128)\n",
        "                      return_attention_mask = True,       # Construct attention masks\n",
        "                      return_tensors = 'pt',              # Return pytorch tensors\n",
        "                      )\n",
        "  \n",
        "  # Append encoded review to list   \n",
        "  X_mapped.append(encoded_dict['input_ids'])\n",
        "  # Append its attention mask to list\n",
        "  att_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert to tensors\n",
        "X_mapped = torch.cat(X_mapped, dim=0)\n",
        "att_masks = torch.cat(att_masks, dim=0)\n",
        "Y = torch.tensor(Y.values)\n",
        "\n",
        "# Check that we now have mapped ids for each word from all reviews stored in list X_mapped (by printing the second review before and after encode_plus)\n",
        "print('Review after preprocessing:', X[1])\n",
        "print('Mapped token IDs:', X_mapped[1])\n",
        "print('Sentiment (Pos = 1, Neg = 0):', Y[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byT7Gw860Qm1"
      },
      "source": [
        "# Divide dataset to a 85-15 split, so that we now have 38,2k mapped reviews for training and 6,8k mapped reviews for validation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YPCeC8gSYPS",
        "outputId": "1f284dbc-97bb-4182-c4d3-564a21c94bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples for training: 38256\n",
            "Samples for validation: 6752\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine training inputs into a TensorDataset\n",
        "dataset = TensorDataset(X_mapped, att_masks, Y)\n",
        "\n",
        "train_size = int(0.85 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(\"Samples for training:\", train_size)\n",
        "print(\"Samples for validation:\", val_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9cr1rOS2exI"
      },
      "source": [
        "# Creating DataLoaders for our training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3MH-9YZxPMIO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# For fine-tuning BERT on a downstream task, documentation recommends a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Using random sampler to take batches in random order\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
        "# For validation batch order doesn't matter, so we take them sequentially\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-WXl3KCNx8a"
      },
      "source": [
        "# Preparation for training: choosing bert model, number of epochs and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6xWic61SHhI",
        "outputId": "d7cb3702-7f07-4aa8-be8c-342b07183de3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Using BertForSequenceClassification, which is the pretrained BERT model with one linear classification layer on top\n",
        "# Setting the number of output layers: 2 for binary classification.\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)\n",
        "\n",
        "# Running the model on the gpu\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "# Tried recommended 2, 3 and 4 epochs\n",
        "# However my model seems to overfit for 3 or more epochs, so:\n",
        "epochs = 2\n",
        "\n",
        "# Tried recommended learning rates: 5e-5, 3e-5, 2e-5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj2ELw_ENRhi"
      },
      "source": [
        "# Train the model and then evaluate it after each epoch using f1 score, precision and recall\n",
        "\n",
        "Also printing total loss for each epoch on the training set and on the validation set, to easily observe cases of overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lil3PmPzTakk",
        "outputId": "03e54b61-fb0e-4550-e32b-88268f412ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 . . .\n",
            "  Batch  200  of  1,196 -- Time passed: 0:02:11.\n",
            "  Batch  400  of  1,196 -- Time passed: 0:04:22.\n",
            "  Batch  600  of  1,196 -- Time passed: 0:06:32.\n",
            "  Batch  800  of  1,196 -- Time passed: 0:08:43.\n",
            "  Batch 1,000  of  1,196 -- Time passed: 0:10:54.\n",
            "\n",
            "  Training time for this epoch: 0:13:01\n",
            "  Average Training Loss: 0.32\n",
            "  Validation Loss: 0.2435\n",
            "\n",
            "  VALIDATION SCORES:\n",
            "\n",
            "  F1 SCORE: 0.9034\n",
            "  PRECISION: 0.8921\n",
            "  RECALL: 0.9151\n",
            "\n",
            "Epoch 2 . . .\n",
            "  Batch  200  of  1,196 -- Time passed: 0:02:11.\n",
            "  Batch  400  of  1,196 -- Time passed: 0:04:21.\n",
            "  Batch  600  of  1,196 -- Time passed: 0:06:32.\n",
            "  Batch  800  of  1,196 -- Time passed: 0:08:43.\n",
            "  Batch 1,000  of  1,196 -- Time passed: 0:10:53.\n",
            "\n",
            "  Training time for this epoch: 0:13:01\n",
            "  Average Training Loss: 0.19\n",
            "  Validation Loss: 0.2600\n",
            "\n",
            "  VALIDATION SCORES:\n",
            "\n",
            "  F1 SCORE: 0.9068\n",
            "  PRECISION: 0.8871\n",
            "  RECALL: 0.9274\n",
            "\n",
            "Total time 0:27:30\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Function that takes a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_time = time.time()\n",
        "\n",
        "for ep in range(0, epochs):\n",
        "  print(\"\\nEpoch\", ep + 1, \". . .\")\n",
        "\n",
        "  t = time.time()\n",
        "  total_train_loss = 0\n",
        "\n",
        "  # ------- TRAINING ---------\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Print progress every 200 batches\n",
        "    if step % 200 == 0 and step != 0:\n",
        "\n",
        "      elapsed_t = format_time(time.time() - t)\n",
        "      print('  Batch {:>4,}  of  {:>4,} -- Time passed: {:}.'.format(step, len(train_dataloader), elapsed_t))\n",
        "\n",
        "    # Unpack this training batch from our dataloader and copy each tensor to the GPU\n",
        "    batch_X_mapped = batch[0].to(device)\n",
        "    batch_input_mask = batch[1].to(device)\n",
        "    batch_labels = batch[2].to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    output = model(batch_X_mapped, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
        "    loss = output[0]\n",
        "    logits = output[1]\n",
        "\n",
        "    total_train_loss += loss.item()\n",
        "        \n",
        "    # backpropagation - compute gradients \n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradient norm to 1.0\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # apply gradients \n",
        "    optimizer.step()\n",
        "\n",
        "  # Calculate the average loss of all batches for this epoch\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "  \n",
        "  training_time = format_time(time.time() - t)\n",
        "  print(\"\\n  Training time for this epoch: {:}\".format(training_time))\n",
        "  print(\"  Average Training Loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "  # ------- EVALUATION ---------\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "  total_eval_loss = 0\n",
        "\n",
        "  predictions , actual_labels = [], []\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    batch_X_mapped, batch_input_mask, batch_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(batch_X_mapped, token_type_ids=None, attention_mask=batch_input_mask, labels = batch_labels)\n",
        "        \n",
        "    loss = output[0]\n",
        "    logits = output[1]\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    batch_labels = batch_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and actual labels\n",
        "    predictions.append(logits)\n",
        "    actual_labels.append(batch_labels)\n",
        "\n",
        "  # Flatten predictions and actual labels from list of batches to 1 list\n",
        "  flat_labels, flat_pred = [], []\n",
        "  for lab in actual_labels:\n",
        "    for prd in lab:\n",
        "      flat_labels.append(prd)\n",
        "  flat_pred = []\n",
        "  for b_pr in predictions:\n",
        "    for prd in b_pr:\n",
        "      flat_pred.append(prd)\n",
        "\n",
        "  # The predictions are a 2-column ndarray. Get index of highest value for each tuple in order to turn it into a list of 0s and 1s\n",
        "  pred_labels = np.argmax(flat_pred, axis=1)\n",
        "  # print(len(flat_labels))\n",
        "  # print(len(pred_labels))\n",
        "\n",
        "  # Calculate and print validation scores\n",
        "  avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "  print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
        "  print(\"\\n  VALIDATION SCORES:\\n\")\n",
        "  f1_val = f1_score(flat_labels, pred_labels)\n",
        "  print(\"  F1 SCORE: {0:.4f}\".format(f1_val))\n",
        "  pscore = precision_score(flat_labels, pred_labels)\n",
        "  print(\"  PRECISION: {0:.4f}\".format(pscore))\n",
        "  rscore = recall_score(flat_labels, pred_labels)\n",
        "  print(\"  RECALL: {0:.4f}\".format(rscore))\n",
        "\n",
        "print(\"\\nTotal time {:}\".format(format_time(time.time()-total_time)))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
